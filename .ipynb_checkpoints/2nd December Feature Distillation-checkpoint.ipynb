{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Distillation in Neural Networks\n",
    "## AKA Learning Using Provileged Information in Neural Networks\n",
    "\n",
    "This came about because I very much misunderstood model distillation. Joe clarified where I was going wrong and helped solidify some of the ideas. I was then going to throw the idea away, assuming it had been done before, but then Chris asked if anyone had any ideas for using privileged information in Neural Networks, which this approach gives you. In speaking to Chris, with Joe we came to think this might be useful. However, I'm pretty sure it's similar to Transfer Learning. Maybe I need to do more reading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What this would look like\n",
    "There are 2 steps to this. Firstly, train a network on all $m$ training data with $n$ standard features and $k$ privileged features, $X_{1 \\dots m}\\{x_0, x_1, \\dots, x_n\\}, x \\in \\mathcal{X}$ and $X^*_{1 \\dots m} \\{x^*_0, x^*_1, \\dots, x^*_k \\}, x^* \\in \\mathcal{X^*}$.\n",
    "\n",
    "This should produce a network like that in the image below.\n",
    "![Prior to distillation](Images/Model Prior To Feature Distillation.png \"Model Prior To Feature Distillation\")\n",
    "\n",
    "To 'distil' the above, we're going to essentially going to copy the inputs to the output layer. But we do this in a kind of backwards way. So once we've learned the big model, we save the weights to the output layer and the output generated by the model, $\\langle h , w \\rangle $. By fixing the weights and setting the output, the only way that we can achieve this output is to learn the inputs to the penultimate layer. See below.\n",
    "\n",
    "![Post distillation](Images/Model During Feature Distillation Alternative.png \"Model During Feature Distillation Alternative\")\n",
    "\n",
    "Post distillation we now have a model we can use with the reduced number of inputs. See below.\n",
    "\n",
    "![Post distillation](Images/Model Post Feature Distillation.png \"Model Post Feature Distillation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "We're going to use Keras, as it makes this kind of prototyping easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Volumes/LocalDataHD/o/ot/ot44/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we're loading the data. It's TechTC data from Joe's LUPI paper where the privileged information is unselected features. The only features that were selected ar those where the values are different in something like 95% of all rows. So there's a lot of unselected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.loadtxt(\"data/fold_0/train_sel_inputs.txt\")\n",
    "train_s = np.loadtxt(\"data/fold_0/train_unsel_inputs.txt\")\n",
    "train_xs = np.hstack((train_x, train_s))\n",
    "train_y = np.loadtxt(\"data/fold_0/train_labels.txt\")\n",
    "\n",
    "valid_x = np.loadtxt(\"data/fold_0/test_sel_inputs.txt\")\n",
    "valid_s = np.loadtxt(\"data/fold_0/test_unsel_inputs.txt\")\n",
    "valid_xs = np.hstack((valid_x, valid_s))\n",
    "valid_y = np.loadtxt(\"data/fold_0/test_labels.txt\")\n",
    "\n",
    "test_x = np.loadtxt(\"data/test_sel_inputs.txt\")\n",
    "test_s = np.loadtxt(\"data/test_unsel_inputs.txt\")\n",
    "test_xs = np.hstack((test_x, test_s))\n",
    "test_y = np.loadtxt(\"data/test_labels.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The BIG Model\n",
    "This is the model that gets trained on all the data and will eventually be distiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_model = Sequential()\n",
    "all_data_model.add(Dense(200, input_dim=train_xs.shape[1]))#, activation='sigmoid'))\n",
    "all_data_model.add(Dropout(0.25))\n",
    "all_data_model.add(Dense(100))#, activation='sigmoid'))\n",
    "all_data_model.add(Dropout(0.25))\n",
    "all_data_model.add(Dense(10))#, activation='sigmoid'))\n",
    "all_data_model.add(Dense(1, activation='tanh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_model.compile(optimizer='sgd', loss='mean_squared_error', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a1d000198>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_model.fit(train_xs, train_y, batch_size=32, epochs=EPOCHS, verbose=0, validation_data=(valid_xs, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "32/40 [=======================>......] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "40/40 [==============================] - 0s 404us/step\n",
      "Loss: 1.19442424774 Acc: 0.675\n"
     ]
    }
   ],
   "source": [
    "score = all_data_model.evaluate(test_xs, test_y, verbose=1)\n",
    "print(\"Loss:\", score[0], \"Acc:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the output as this is needed for distilling the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 1.], dtype=float32), -1.0),\n",
       " (array([-0.74228102], dtype=float32), 1.0),\n",
       " (array([ 0.99999911], dtype=float32), -1.0),\n",
       " (array([-0.99987549], dtype=float32), 1.0),\n",
       " (array([-1.], dtype=float32), 1.0),\n",
       " (array([ 1.], dtype=float32), -1.0),\n",
       " (array([ 1.], dtype=float32), -1.0),\n",
       " (array([-0.99986571], dtype=float32), 1.0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y = all_data_model.predict(train_xs)\n",
    "\n",
    "valid_pred_y = all_data_model.predict(valid_xs)\n",
    "#valid_pred_y[valid_pred_y < 1] = -1\n",
    "[(valid_pred_y[i], valid_y[i]) for i in range(len(valid_y)) if not(np.round(valid_pred_y[i]) == valid_y[i])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Distilled Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilled_model = Sequential()\n",
    "distilled_model.add(Dense(200, input_dim=train_x.shape[1]))\n",
    "distilled_model.add(Dropout(0.25))\n",
    "distilled_model.add(Dense(100))\n",
    "distilled_model.add(Dropout(0.25))\n",
    "distilled_model.add(Dense(10))\n",
    "distilled_model.add(all_data_model.get_layer(index=6))\n",
    "distilled_model.get_layer(index=6).trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilled_model.compile(optimizer='sgd', loss='mean_squared_error', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a1d76b240>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilled_model.fit(train_x, pred_y, batch_size=32, epochs=EPOCHS, verbose=0, validation_data=(valid_x, valid_pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "32/40 [=======================>......] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "40/40 [==============================] - 0s 97us/step\n",
      "Loss: 1.27045834064 Acc: 0.675\n"
     ]
    }
   ],
   "source": [
    "score = distilled_model.evaluate(test_x, test_y)\n",
    "print(\"Loss:\", score[0], \"Acc:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just demonstrate that the last layer of both models is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.10814332],\n",
       "       [-0.73108137],\n",
       "       [-0.24628025],\n",
       "       [-0.30259526],\n",
       "       [-0.20805749],\n",
       "       [ 0.05454459],\n",
       "       [-0.6163854 ],\n",
       "       [ 0.53691632],\n",
       "       [ 0.46061108],\n",
       "       [-0.52381009]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_model.get_weights()[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.10814332],\n",
       "       [-0.73108137],\n",
       "       [-0.24628025],\n",
       "       [-0.30259526],\n",
       "       [-0.20805749],\n",
       "       [ 0.05454459],\n",
       "       [-0.6163854 ],\n",
       "       [ 0.53691632],\n",
       "       [ 0.46061108],\n",
       "       [-0.52381009]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilled_model.get_weights()[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Model\n",
    "A normal setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_model = Sequential()\n",
    "regular_model.add(Dense(200, input_dim=train_x.shape[1]))\n",
    "regular_model.add(Dropout(0.25))\n",
    "regular_model.add(Dense(100))\n",
    "regular_model.add(Dropout(0.25))\n",
    "regular_model.add(Dense(10))\n",
    "regular_model.add(Dense(1, activation='tanh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_model.compile(optimizer='sgd', loss='hinge', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2075ac18>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_model.fit(train_x, train_y, batch_size=32, epochs=EPOCHS, verbose=0, validation_data=(valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "32/40 [=======================>......] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "40/40 [==============================] - 0s 113us/step\n",
      "Loss: 0.720061767101 Acc: 0.625\n"
     ]
    }
   ],
   "source": [
    "score = regular_model.evaluate(test_x, test_y)\n",
    "print(\"Loss:\", score[0], \"Acc:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, just to demonstrate that the last layer's weights are different in this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4607977 ],\n",
       "       [ 0.59714705],\n",
       "       [ 0.21520783],\n",
       "       [ 0.48031554],\n",
       "       [ 0.73372406],\n",
       "       [ 0.60548145],\n",
       "       [-0.47019443],\n",
       "       [-0.75899076],\n",
       "       [ 0.11630869],\n",
       "       [ 0.72340775]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_model.get_weights()[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Interesting Bit\n",
    "The thing that I find really interesting is if we do it the other way around. So we only have useless information at deployment time, not useful data. And this method looks lke it might work. Maybe..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The BIG Model\n",
    "This is the model that gets trained on all the data and will eventually be distiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_model_inverse = Sequential()\n",
    "all_data_model_inverse.add(Dense(200, input_dim=train_xs.shape[1]))\n",
    "all_data_model_inverse.add(Dropout(0.25))\n",
    "all_data_model_inverse.add(Dense(100))\n",
    "all_data_model_inverse.add(Dropout(0.25))\n",
    "all_data_model_inverse.add(Dense(10))\n",
    "all_data_model_inverse.add(Dense(1, activation='tanh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_model_inverse.compile(optimizer='sgd', loss='mean_squared_error', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a23af18d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_model_inverse.fit(train_xs, train_y, batch_size=32, epochs=EPOCHS, verbose=0, validation_data=(valid_xs, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "32/40 [=======================>......] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "40/40 [==============================] - 0s 446us/step\n",
      "Loss: 1.09190224409 Acc: 0.65\n"
     ]
    }
   ],
   "source": [
    "score = all_data_model_inverse.evaluate(test_xs, test_y)\n",
    "print(\"Loss:\", score[0], \"Acc:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_inverse = all_data_model_inverse.predict(train_xs)\n",
    "\n",
    "valid_pred_y_inverse = all_data_model_inverse.predict(valid_xs)\n",
    "#valid_pred_y_inverse[valid_pred_y_inverse < 1] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Distilled Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilled_model_inverse = Sequential()\n",
    "distilled_model_inverse.add(Dense(200, input_dim=train_s.shape[1]))\n",
    "distilled_model_inverse.add(Dropout(0.25))\n",
    "distilled_model_inverse.add(Dense(100))\n",
    "distilled_model_inverse.add(Dropout(0.25))\n",
    "distilled_model_inverse.add(Dense(10))\n",
    "distilled_model_inverse.add(all_data_model_inverse.get_layer(index=6))\n",
    "distilled_model_inverse.get_layer(index=6).trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilled_model_inverse.compile(optimizer='sgd', loss='mean_squared_error', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2deaa940>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilled_model_inverse.fit(train_s, pred_y_inverse, batch_size=32, epochs=EPOCHS, verbose=0, validation_data=(valid_s, valid_pred_y_inverse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "32/40 [=======================>......] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "40/40 [==============================] - 0s 449us/step\n",
      "Loss: 0.899837994576 Acc: 0.6\n"
     ]
    }
   ],
   "source": [
    "score = distilled_model_inverse.evaluate(test_s, test_y)\n",
    "print(\"Loss:\", score[0], \"Acc:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just demonstrate that the last layer of both models is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.34592503],\n",
       "       [-0.43121678],\n",
       "       [ 0.01841524],\n",
       "       [-0.03538501],\n",
       "       [ 0.23351964],\n",
       "       [ 0.31625515],\n",
       "       [ 0.08129516],\n",
       "       [ 0.24874951],\n",
       "       [-0.59812289],\n",
       "       [ 0.25800267]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_model_inverse.get_weights()[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.34592503],\n",
       "       [-0.43121678],\n",
       "       [ 0.01841524],\n",
       "       [-0.03538501],\n",
       "       [ 0.23351964],\n",
       "       [ 0.31625515],\n",
       "       [ 0.08129516],\n",
       "       [ 0.24874951],\n",
       "       [-0.59812289],\n",
       "       [ 0.25800267]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilled_model_inverse.get_weights()[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Model\n",
    "A normal setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_model_inverse = Sequential()\n",
    "regular_model_inverse.add(Dense(200, input_dim=train_s.shape[1]))\n",
    "regular_model_inverse.add(Dropout(0.25))\n",
    "regular_model_inverse.add(Dense(100))\n",
    "regular_model_inverse.add(Dropout(0.25))\n",
    "regular_model_inverse.add(Dense(10))\n",
    "regular_model_inverse.add(Dense(1, activation='tanh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_model_inverse.compile(optimizer='sgd', loss='mean_squared_error', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a28878ef0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_model_inverse.fit(train_s, train_y, batch_size=32, epochs=EPOCHS, verbose=0, validation_data=(valid_s, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "32/40 [=======================>......] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "40/40 [==============================] - 0s 396us/step\n",
      "Loss: 0.969038486481 Acc: 0.5\n"
     ]
    }
   ],
   "source": [
    "score = regular_model_inverse.evaluate(test_s, test_y)\n",
    "print(\"Loss:\", score[0], \"Acc:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, just to demonstrate that the last layer's weights are different in this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.42132613],\n",
       "       [-0.64012021],\n",
       "       [ 0.52992225],\n",
       "       [-0.41942331],\n",
       "       [-0.07013734],\n",
       "       [ 0.798958  ],\n",
       "       [ 0.3937121 ],\n",
       "       [ 0.1461388 ],\n",
       "       [ 0.43921226],\n",
       "       [-0.04010696]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_model_inverse.get_weights()[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
